## **Abstract**

Words do not carry fixed meanings in isolation – their meaning emerges from how they are used in sentences and situations. This essay proposes the **“Rod & Mushroom” framework**, a philosophically inspired model of semantic structure that bridges classical theories of meaning and modern language technology. Grounded in insights from Wittgenstein’s _meaning-as-use_ philosophy and Quine’s _semantic holism_, as well as phenomenological perspectives from Husserl and Merleau-Ponty on embodiment, the framework posits that each word’s meaning has a **literal core** and an **extended conceptual facet**. We distinguish literal, physically grounded usages (denoted as **P** usages) from idiomatic or abstract usages (denoted as **Q** usages) for words, using a custom classifier to identify each in a large corpus. By mapping these usages into a high-dimensional semantic space with BERT-based sentence embeddings, we can visualize each word as a _“mushroom”_ – with a sturdy **stem** of literal meaning and a broad **cap** of conceptual associations. The overlapping of these “mushroom caps” between words reveals how concepts relate and cluster. Our results show that this structure is empirically detectable: for example, concrete terms like _“cat”_ and _“tomato”_ have distinct, non-overlapping Q-usage contexts, while abstract terms like _“matrix”_ and _“vector”_ share large portions of their Q-usage contexts. This work demonstrates an interpretable way to connect philosophical semantics with state-of-the-art language models, shedding light on how large language models (LLMs) might represent meaning through use and context. It also contributes to ongoing debates on grounding AI language in human experience, suggesting that even without direct perception, an LLM’s understanding can be scaffolded by usage patterns.

## **Introduction**

What does a word _mean_? Philosophers and linguists have long argued that a word’s meaning is not an intrinsic property but is determined by **its use in context** . Ludwig Wittgenstein famously stated that “the meaning of a word is its use in the language,” emphasizing that language derives its meaning from practical contexts and **language-games** rather than static definitions . Willard Quine, in turn, advanced a view of **semantic holism**: the idea that words and sentences only gain meaning in the network of relationships with other sentences – in Quine’s words, _“the meaning of a sentence is determined by its relationship to other sentences in a linguistic system, rather than by reference to external objects”_ . In other words, our beliefs and meanings form a “web of belief,” and a single word’s content cannot be isolated from the totality of language . From the phenomenological side, thinkers like Edmund Husserl and Maurice Merleau-Ponty stressed that meaning is rooted in our **intentional and embodied experience** of the world. Merleau-Ponty argued that human consciousness and language remain anchored in bodily perception – even our highest abstractions are “never detached from \[their] moorings in a concrete and embodied situation” . Language, he suggested, initially emerges as embodied gesture and only later solidifies into abstract symbols . These converging insights from analytic philosophy (meaning-as-use, holism) and phenomenology (embodiment and intentionality) suggest that _words derive their significance from how they are used within structured contexts of sentences and human activities_. A word’s “meaning” encompasses both a direct reference (often physical or literal) and a position in a larger conceptual network of usage and inference.

Modern computational semantics has inadvertently embraced these principles. **Distributional semantics** – the approach underpinning today’s word embeddings and language models – operationalizes Wittgenstein’s idea by inferring meaning from usage patterns in large text corpora . The popular adage by Firth, _“You shall know a word by the company it keeps,”_ encapsulates this approach, and indeed neural embedding models like word2vec or BERT learn vector representations by observing which words co-occur in which contexts . These vectors encode rich information about a word’s associations and usage. In fact, word and sentence embeddings can be seen as constructing a **“conceptual space”** for language: a geometric space where proximity reflects semantic similarity or relatedness . Such conceptual space frameworks have been used in cognitive science to align linguistic patterns with cognitive representations . Recent studies show that **large language models** (LLMs) exhibit an impressive ability to capture semantic relationships in these high-dimensional spaces. For example, a pre-trained BERT model can place different senses of a polysemous word into distinct regions of its embedding space . This means that, without any human supervision, the model’s internal representations for, say, _“bank”_ in _“river bank”_ versus _“central bank”_ end up in different clusters – a striking confirmation that meaning is emergent from usage context . Advances like **Sentence-BERT (SBERT)** further refine this by producing semantically coherent sentence embeddings , allowing us to compare whole sentence meanings. SBERT was a “milestone” in that it fine-tunes BERT to yield vector representations where distances correspond better to human-perceived semantic similarity . In short, contemporary NLP not only echoes the philosophical view that meaning lives in use and context, but also provides tools to **quantify and visualize** these usage-based meanings.

Despite these advances, a core challenge remains: words are **multifaceted**. Many words straddle a concrete, literal meaning and one or more abstract or metaphorical meanings. Consider the word “fork.” In a literal sense, a _fork_ is a utensil with prongs (a physical object); in a conceptual sense, _“a fork in the road”_ is a branching point (an abstract idea of divergence). Distributional models mix all usages of “fork” into one vector (or one set of closely related vectors), which can make interpretation difficult. It can be hard to tell what part of that representation corresponds to the physical fork or the conceptual “fork.” To disentangle such cases, researchers have looked at contextualized embeddings which can separate senses , and even at clustering techniques for **word sense induction**. Our approach builds on this idea by explicitly **classifying usage types**. We introduce a custom **role classifier** that labels each occurrence of a target word in a corpus as either a **P (literal/physical)** usage or a **Q (idiomatic or conceptual)** usage, based on cues like the surrounding words and syntactic patterns. This P/Q distinction is inspired by the philosophical separation between what Husserl called the “intuition” tied to an expression (a direct presentation of an object or experience) and the freely varying “meaning intention” that can detach from any specific instance . In simpler terms, P-uses of a word point to concrete things or straightforward literal meanings (what Quine might consider observation-linked usages), whereas Q-uses invoke conceptual, metaphorical, or context-dependent meanings that often transcend the here-and-now (what Wittgenstein would describe as part of a language-game’s rules). For each word of interest, we thus obtain two sets of sentences: one set where the word is used in its P role and another where it’s used in its Q role.

Using state-of-the-art **BERT-based sentence embeddings**, specifically SBERT, we project each sentence (context) into a high-dimensional vector space that reflects its semantic content . This allows us to analyze the **geometry of a word’s usage**. In this space, each word’s P-usage sentences tend to form one cluster and its Q-usage sentences form another cluster. We refer to these clusters metaphorically as the **“stem”** and **“cap”** of a semantic **mushroom**. The _stem_ is the narrow, grounded cluster of literal uses, presumably rooted in perceptual experience (for example, sentences about “fork” as a physical utensil will cluster together). The _cap_ is the broader, flatter cluster of extended or figurative uses (e.g. sentences about “fork” in abstract contexts like decision-making or version control in software). We call our overall model the **Rod & Mushroom framework** to highlight two aspects: the **rod** suggests a unifying structure that runs through a word’s various uses (akin to a backbone or stem that connects meaning facets), and the **mushroom** symbolizes the two-part structure (a stalk and a cap). Figure 1 (hypothetical in this text) would illustrate this concept, showing, for example, the word “fork” with a stem cluster in one region (literal uses involving eating, cooking, etc.) and a cap extending out (conceptual uses like “fork in the road,” “fork a repository”) separated along some principal component that differentiates concrete vs. abstract contexts.

Crucially, when we compare the _caps_ of different words, we gain insight into how concepts overlap or diverge. In our experiments, we quantified the intersections of Q-usage sentence sets between words. The results align with intuition and philosophical expectation. **Concrete nouns** that refer to physical objects or beings – _cat_, _tomato_, _fork_, etc. – had virtually no overlap in their Q usages: each forms its own separate “mushroom cap” with idiosyncratic idioms or associations (a cat has nothing to do with a tomato in abstract context, and indeed our data showed only 1 overlapping Q-sentence out of thousands). In contrast, more **abstract or technical terms** showed significant overlap. For instance, the conceptual contexts of _“matrix”_ and _“vector”_ overlapped heavily (hundreds of shared sentences), reflecting the fact that in extended usage these words often co-occur in similar discussions (e.g. linear algebra or computer graphics scenarios). _“Function”_ and _“vector”_ also shared some Q-contexts (both appearing in mathematical or computational narratives), though less so than _matrix–vector_. This paints a picture of a **conceptual space** where abstract concepts form interconnected regions – akin to how a semantic network or an ontology would link related concepts – while concrete concepts remain more isolated unless bridged by higher-level analogies. Such findings resonate with cognitive semantic theories that distinguish **perceptually grounded meanings** from **conceptual metaphors**: our literal _stem_ clusters are tied closely to perceptual experience (grounded meanings), and the overlaps in _cap_ clusters hint at shared metaphors or conceptual blends between domains (e.g., the idea of a “matrix” linking to a “network” or “grid” concept that might also underlie “vector” usage). Notably, this framework provides a **visual and quantitative** method to explore these ideas. By using techniques like principal component analysis (PCA) on the embedding vectors, we can plot the relationships: indeed, we generated 2D and 3D plots where each word’s P and Q usage clusters appear as distinct shapes (the mushrooms), and one can literally see the overlaps where caps coincide.

From a broader perspective, the Rod & Mushroom framework contributes to both the **technical understanding of LLM semantics** and the **philosophical discourse on meaning**. Technically, it offers an interpretable layer on top of opaque neural embeddings: rather than treating a word vector as an inscrutable bundle of statistics, we decompose a word’s representation into meaningful parts linked to identifiable types of usage. This interpretable structure could aid in **model diagnostics** (e.g. understanding how and why an LLM might confuse two concepts) and in **embedding alignment tasks** (for example, aligning a model’s literal usage vectors with images or sensor data for better grounding). Philosophically, our results lend empirical support to the view that **meaning is use** , showing that even in a machine learning model, a word’s “meaning” can be seen as a structure arising from its various uses. At the same time, the distinction between P and Q usages echoes the **symbol grounding problem**: how do abstract symbols get meaning in the first place? The fact that we must identify a literal, grounded usage (P) for each word acknowledges that at least part of meaning is tied to something outside language – typically, our physical or social experience . The Q usages, which are “grounded” only indirectly (via other words and contexts), are _parasitic_ on those literal cores , yet they form the majority of our conceptual world. Our framework shows how an LLM, armed only with text, negotiates this gap: it uses the co-occurrence structure of language to build a _virtual grounding_ of concepts in relation to each other. While an AI might not have _direct_ sensory grounding for “cat” or “matrix,” the way those words are used across thousands of sentences provides a proxy to grounding – a rich inferential role in a semantic network . This intersects with current debates on whether LLMs “understand” or merely manipulate symbols. On one hand, our analysis highlights that LLMs encode usage-based meaning very effectively (clustering senses, capturing analogies), aligning with an **inferentialist** view of meaning (understanding as the ability to infer and relate concepts). On the other hand, the necessity of an initial P “stem” for grounding hints that truly robust understanding may require connection to the _world_, not just words – a point raised by many cognitive scientists and AI researchers concerned with grounding and intentionality .

In summary, this essay marries classical philosophy of language with NLP to propose a novel way of structuring word meaning. The **Rod & Mushroom model** provides:

* **A philosophically grounded schema** – drawing on Wittgenstein’s use-theory of meaning, Quine’s holism, and phenomenological grounding – to differentiate components of word meaning (literal vs. conceptual).

* **An empirical methodology** – using a P/Q classifier and SBERT embeddings – to instantiate this schema on actual corpus data, yielding interpretable visualizations of each word’s “mushroom” semantic structure.

* **Insights into LLM semantics** – we demonstrate that large language models, through usage patterns alone, create conceptual spaces that mirror human-like semantic distinctions (e.g., distinguishing senses, grouping related abstract concepts) . This suggests that LLMs implicitly solve parts of the grounding puzzle by relying on the structure of language itself.

* **Relevance to grounding and understanding debates** – by showing how abstract meanings relate to literal ones in the model, we provide a framework to discuss what it means for an AI to _understand_ a concept. The overlaps of “mushroom caps” can be seen as a proxy for conceptual common ground between words (e.g., the model knows _“red”_ and _“blue”_ share a domain of color, whereas _“red”_ and _“cat”_ do not), while the stems ensure each concept has an anchor point.

The remainder of this essay will delve deeper into how the Rod & Mushroom framework is constructed, present the detailed results of our case studies (with figures depicting the semantic mushrooms for various example words), and discuss implications for future research. We will also compare this approach to other contemporary efforts to interpret embedding spaces and to multi-modal approaches that attempt to ground language in vision or robotics. By the end, we hope to convince the reader that a synthesis of **philosophical insight and AI methodology** can yield both a clearer understanding of linguistic meaning and a more interpretable, human-aligned language model.


## **Philosophical Perspectives on Meaning and Grounding**

### **Analytic Views: Meaning as Use and Holistic Context**

In analytic philosophy, one influential view is that the meaning of words arises from their **use in sentences and contexts** rather than from fixed definitions. Ludwig **Wittgenstein** famously stated that _“the meaning of a word is its use in the language.”_ In his _Philosophical Investigations_ (1953), Wittgenstein argued that words don’t have meaning in isolation; instead, meaning emerges from the myriad ways words are used within _language-games_ – the goal-directed activities and contexts into which language is woven . For example, the word _“table”_ means what it does because of how people use it in practical situations involving tables. Notably, Wittgenstein emphasized that these language-games include not only words but also non-linguistic actions and contexts . Words function like tools within human activity , suggesting that linguistic meaning is anchored in our interactions with the world. This idea – that _use anchors meaning_ – anticipates the modern distributional idea that context defines a word. Indeed, Wittgenstein’s insight that _“words get their meaning by virtue of their relationships with other words and their use”_ aligns with the principle that you “know a word by the company it keeps” in language .

Willard Van Orman **Quine** pushed analytic philosophy toward an even more holistic view. Quine argued that _the unit of meaning is not the single word or even sentence, but the entire network of sentences that constitute a theory or language_. In _Two Dogmas of Empiricism_ (1951) and _Word and Object_ (1960), Quine rejected the idea that each statement has an isolated meaning verified by experience. Instead, _“sentences do not get verified in isolation”_ and _“language as a whole has to get its meaning by an encounter with the world.”_ In other words, our _web of beliefs_ (all our sentences and their interrelations) collectively face the tribunal of experience. Any given word’s meaning is underdetermined by experience (“_gavagai_” could mean rabbit, or undetached rabbit-part, etc.), and only the total theory constrains its reference. This is Quine’s **meaning holism** and the doctrine of the **inscrutability of reference**. A consequence is that _overlap in meaning_ between words is also holistic: two terms share meaning insofar as they play similar roles in the network of sentences that is confirmed or disconfirmed by experience. Quine’s view contrasts with Wittgenstein’s focus on local usage, yet both converge on the idea that meaning is not a static correspondence between word and object, but emerges from _patterns of use_ – either local (Wittgenstein’s language-games) or global (Quine’s web of sentences). Notably, Quine also stressed that language _must connect to sensory input somewhere_ (through observation sentences), though not word-by-word but globally . This resonates with the idea that an abstract semantic space (a network of meanings) still needs _grounding in a sensory plane_, albeit at the level of overall consistency with experience.

### **Phenomenological Views: Intentionality, Perception, and Embodiment**

In contrast to analytic focus on language-in-use, **phenomenological** philosophers examine how meaning is founded on experience and consciousness. **Edmund Husserl**, the founder of phenomenology, introduced the concept of _intentionality_: consciousness is always _of_ or _about_ something, and thus every act of thought has a content or meaning. Husserl distinguished between an “empty” intention (a thought of an object in its absence) and a “fulfilled” intention where the object is directly intuited or perceived . His analyses in _Logical Investigations_ (1900) and later works detail how _concepts and meanings are “constituted”_ in the mind by abstracting and synthesizing across particular experiences. Importantly, Husserl offered _one of the few detailed accounts of how a perceptual experience can transform a mere thought into knowledge_ . In his view, _perception provides an intuitive fulfillment_ that gives concepts their true content. For example, one might have the concept _“red”_ as a meaning intention, but seeing an actual red object _fulfills_ that intention, grounding the concept in a direct experience. This is essentially a **phenomenological grounding** of meaning: even abstract words ultimately get their sense from potential or actual experiences. Without such fulfillment, meanings remain unanchored.[^4] Husserl thus would liken purely verbal or symbolic meanings to “empty” shells that need _sensory or intuitive content_ to be complete.

Maurice **Merleau-Ponty**, another key phenomenologist, further emphasized the role of the **body and perception** in meaning. In _Phenomenology of Perception_ (1945), Merleau-Ponty argued that our body is our general medium for having a world, and that perception is fundamentally about _interaction with things_, not passive reception of data. Consequently, _meaning is not a detached mental label; it is woven into the fabric of perceptual life_. Even abstract thought, he suggested, is built upon a foundation of bodily experience. For Merleau-Ponty, we first grasp significance through our _embodied interactions_ (e.g. the way a door _“invites”_ pushing or a melody _“wants”_ to resolve). Language and higher concepts extend from this primordial layer of meaning. In effect, _the body “expresses meaning” on its own_ – our gestures, orientations, and perceptions already have significance prior to any explicit intellectual analysis. This stance implies that the _semantic space of concepts is implicitly anchored in a sensory-motor space_: what words mean to us is ultimately tied to how we _experience_ and _act_. Thus, where analytic philosophers saw meaning in linguistic usage, Merleau-Ponty sees a deeper grounding: e.g. the concept _“vertical”_ might be anchored in our upright posture and visual field, or _“rough”_ in our tactile experience. Such **embodied meaning** aligns with modern ideas of _grounded cognition_ – the view that conceptual representations are partly simulators of sensory-motor states.

In summary, classic philosophers provide two complementary perspectives: The analytic tradition (Wittgenstein, Quine) highlights _use and context_ – words gain meaning from how they are used in sentences and how sentences relate to each other and experience. The phenomenological tradition (Husserl, Merleau-Ponty) highlights _experience and embodiment_ – concepts gain meaning from their relationship to our perceptions and bodily engagement with the world. Both perspectives suggest that meaning lives in _patterns or relations_ (linguistic or experiential) rather than static word-object mappings. These ideas set the stage for understanding how contemporary AI models represent and perhaps _implicitly_ mirror such patterns.

## **LLMs and Semantic Representations in NLP**

### **Distributional Semantics and Sentence Embeddings**

In natural language processing, the principle that _“meaning is use”_ is operationalized by **distributional semantics**: words and sentences are characterized by the contexts in which they appear. This idea dates back to linguist J.R. **Firth**, who echoed Wittgenstein by saying _“You shall know a word by the company it keeps.”_ In practice, modern LLMs like GPT or BERT are trained on massive corpora of text, adjusting their internal representations to predict or encode words in context. The result is that each word is represented as a vector (a point in a high-dimensional space) that implicitly captures the _patterns of co-occurrence_[^5] with other words. Two words that often appear in similar sentences (e.g. _“doctor”_ and _“nurse”_) end up with vector representations that are close together or have overlapping regions of influence, indicating their meanings are related. In effect, LLM training _anchors words in the “cloud” of sentences they occur in_. As one commenter observed, LLMs have **“absorbed contexts”** from training text such that when they use a word, they use it in ways consistent with how humans do . This is why an LLM can produce seemingly meaningful text: it deploys words in familiar patterns learned from usage. According to a Wittgensteinian view, this is precisely what it means to know the meaning of a word – to use it appropriately in context .

Early approaches to distributional semantics used counts or co-occurrence statistics (e.g. **Latent Semantic Analysis** in the 1990s) to embed words and sentences in a semantic space. The 2010s saw a leap with neural embeddings: **word2vec** (Mikolov et al. 2013) learned word vectors that famously encoded analogical relationships (e.g. the vector arithmetic _king – man + woman ≈ queen_ captured a gender relation). Later, **contextual** models like **BERT** (Devlin et al., 2018) and **GPT** produced _different_ embeddings for a word depending on the sentence, acknowledging that usage varies by context. To obtain a single vector for an entire sentence (a _sentence embedding_), researchers developed models such as **Skip-Thought** (Kiros et al., 2015), **InferSent** (Conneau et al., 2017), and **Universal Sentence Encoder** (Cer et al., 2018). A notable recent model is **Sentence-BERT (SBERT)**, which modifies BERT with a Siamese network structure to produce _“semantically meaningful sentence embeddings that can be compared using cosine-similarity.”_ These sentence embeddings aim to place sentences with similar meaning close together in vector space (for example, _“A dog is chasing a man”_ and _“A man is being chased by a dog”_ should end up with similar embeddings). In distributional terms, each sentence’s vector is informed by the words and structures it contains, so it captures a _combined meaning_.

### **Geometry and Topology of Embedding Spaces**

An intriguing aspect of LLM semantics is the **geometry of the embedding space**. Because embeddings are high-dimensional, researchers often use techniques like PCA or t-SNE to project them into 2D for visualization. What emerges are **clusters or “conceptual clouds”** of representations. Each concept – often anchored by a particular word – corresponds to a region in the space populated by the embeddings of sentences that use that concept. Crucially, these regions are not disjoint; they overlap with neighboring concepts, reflecting gradients of meaning. For instance, the concept _“matrix”_ in a mathematical context might form a cloud that overlaps with the cloud for _“vector”_ (since many sentences discuss both matrices and vectors, indicating shared context/meaning), while also overlapping with _“function”_ or _“square”_ to lesser degrees, forming a whole constellation of related math terms.

(_Visualization of conceptual “caps” and “stems”: PCA projection of sentence embeddings for selected mathematical terms, showing overlapping conceptual clouds (“word caps”) around each anchor word. Each colored blob represents the region of sentence-vector space (the “cap”) where that term appears; overlaps between caps (e.g. vector and matrix) indicate shared usage and thus semantic affinity._)

In the figure above, drawn from an analysis of math text[^6], we see how words like **vector**, **matrix**, **function**, and **square** each correspond to a cluster of sentence embeddings (caps) with significant overlap. The word itself (the label) serves like a **“stem”** or central anchor for its cloud. This illustrates the idea that _the meaning of a word is a cloud of points (uses) in a semantic space_, and _overlap between clouds denotes related meaning_. In computational terms, this is just how distributional semantics plays out: similar contexts = nearby vectors. Research has shown that such embedding spaces are not arbitrary but have meaningful structure. For example, certain _directions_ in the space correspond to consistent semantic changes (a famous example being a gender direction separating male from female terms, or a past-present direction separating _“walk”_ from _“walked”_). Words that share semantic features end up forming discernible subspaces. Moreover, there are global geometric properties: embedding spaces tend to be **anisotropic** (some directions have much more variance than others), and often a few dimensions or principal components capture broad distinctions like topic domains.

Beyond simple visualization, more formal analyses have been applied. Techniques from **topological data analysis** (TDA) have been used to study the shape of BERT’s representation space . For instance, one can compute _persistent homology_ of the point cloud of embeddings to detect how clusters connect or whether there are donut-like holes corresponding to cyclical structures in meaning. While this is cutting-edge and complex, the upshot is that _LLM embedding spaces have a rich geometry that mirrors the semantic relationships between concepts_. Well-separated clusters indicate distinct concepts; contiguous or intersecting regions indicate continuum or ambiguity between concepts. This resonates with philosophical ideas: one might compare it to Wittgenstein’s notion of _“family resemblances”_ where meanings overlap without sharp boundaries – precisely what we see in these vector spaces.

### **Interpretability and Grounding: From Neural Representations to Semantics**

A key question is how these abstract vector representations connect to human-understandable semantics or to the real world. This touches on **interpretability** (can we interpret what dimensions or neurons represent?) and **grounding** (does the model’s internal semantics link to the external, perceptual world?).

On the interpretability front, researchers have found that sometimes individual neurons or directions in LLMs can correspond to intuitive categories. In vision models, for example, _“concept neurons”_ have been discovered (a neuron that fires for _“cat”_ images, or even multi-modal concepts like _“Spider-Man”_ across text and image). In language models, interpretability is more challenging due to distributed representations, but techniques exist to probe what information is encoded. For instance, one can train diagnostic classifiers on the embeddings to see if properties like _part-of-speech_, _tense_, or _topic_ are linearly decodable. These studies show that _lower layers of models encode more syntactic details, while higher layers encode more semantic information_, suggesting an internal progression from structural to conceptual representation. There have also been observations aligning LLM vectors with human semantic knowledge: e.g. certain dimensions might correspond to abstract notions like _positive vs. negative sentiment_, or _concreteness_, etc., learned implicitly from the statistical structure of language.

The question of **grounding** – how language connects to the world of experience – is where philosophy’s influence is strongly felt. A known challenge is the **symbol grounding problem** (Harnad 1990): how can symbols or words acquire meaning _other than by being grounded in something outside the symbol system_, like perception? Pure text-trained LLMs, no matter how large, learn only from word co-occurrences, not from direct sensory input. This leads to concerns that their understanding is shallow – they manipulate symbols (words) without _true reference_. Evidence of this is seen when LLMs are asked to do something requiring real-world interaction or perception. For example, if you ask a vanilla LLM to _“Show me a picture of a cat”_ or _“Please hand me the book”_, it might respond with a literal textual answer (_“Here is a picture of a cat: \[some description]”_) without any awareness that a real action or image is expected . The model lacks grounding to realize that _“showing”_ a picture is a non-linguistic act. This limitation was noted by commentators: _LLMs are very poor at corresponding language to non-linguistic objects or actions_ without special training . In Wittgensteinian terms, the LLM is missing part of the language-game – the practical, non-textual component.

To address this, recent research is increasingly integrating **perceptual data and action** with language learning. Multimodal models like **CLIP** (Radford et al., 2021) are trained on image-text pairs, forcing the embedding of a caption to align with the embedding of the corresponding image. The result is a shared semantic space for words and visuals – e.g., the word _“cat”_ and an image of a cat map to similar vectors. This _implicitly grounds_ the word in visual perception. In robotics and embodied AI, researchers like Bisk et al. (2020) have argued for _“experience grounds language,”_ demonstrating that an agent learning to follow instructions in a simulated environment acquires more robust, grounded word meanings. For instance, a robot that learns the word _“grasp”_ through physical trial-and-error with objects will have a grounded concept of _“grasp”_ tied to sensorimotor experience, unlike an LLM that might only know abstract linguistic associations. These efforts echo **Merleau-Ponty’s** insight – meaning comes alive when rooted in embodiment – and **Husserl’s** idea that perception fulfills meaning. An LLM augmented with a vision module or embodied in a robot is essentially attempting to supply that _fulfilling intuition_ for its otherwise text-bound symbols.

There is also a fascinating feedback loop between philosophy and AI here: Wittgenstein’s “meaning is use” might seem to vindicate LLMs (since they indeed capture usage patterns), but his requirement that meaning includes use _within forms of life_ (practical contexts) points to why LLMs can still fall short. Likewise, Quine’s holism is evident in LLMs – they learn a holistic model of language – but Quine would note they lack the _“encounter with the world”_ that true meaning requires . Thus, interpretability and grounding research in NLP can be seen as trying to bridge the gap: to connect the **“conceptual clouds”** in vector space with the **world of experience**. We see this in work on aligning language model representations with neural data from human brains (e.g., comparing fMRI activation patterns with text embeddings to see if similar semantic structures emerge), and in attempts to inject factual or experiential grounding into models (for example, connecting a model to a database of images or to sensors in real time).

## **III. The Mushroom‑Rod Model: Experiments and Findings**

### The Main Ideas

- o-w and w-c
- Proust and Quine type of sentences
  

### **A. Data Harvesting and Cleaning**

* Harvested two corpora of sentences containing each of 20 target words:

  * **Proust set (P):** sentences rich in sensory, perceptual detail.

  * **Quine set (Q):** sentences emphasizing abstract, conceptual uses.

* Cleaned, deduplicated, and aligned sentences by word via `clean_by_word` and `labels_clean`.

### **B. Semantic Axes and Constructing Them**

* Any object in a LLM (token, word, sentence) is actually a vector in a very high dimensionality space. In our case BERT uses a 768D space. There is a lot of research about how these objects form patterns of association usually in a much lower dimensional space, from 5-20D. This is like when we draw plane figures (a circle, a square) on a paper - this is happening in a 3D space, however these figures are bassically 1D. Patterns in lower dimensions can "live" embedded in much higher dimensionality spaces.
* One idea behind this article is that there is a 5D space inside the sentence embedding space, where some specific patterns of sentences appear. The "axes" of this space are not thought in classical terms (grammar, field of use etc) but more in functional opposed pairs: thing vs concept, literal vs metaphorical, perceived vs symbolic, quantity vs quality, agent vs object. Later on we'll see that except thing vs concept, all the other axis are arbitrary to a point.
* For each of five conceptual contrasting axes:

  1. Wrote 15 prototype sentences per side (e.g. 15 “thing” vs. 15 “concept”).

  2. Embedded with BERT’s \[CLS] token and computed mean vectors.

  3. Defined each axis as the normalized difference `mean(P) – mean(Q)`.

* Saved these axes (`cache/*.npy`) for projections.
* This was done in order to have a first evaluation benchmark to rate other sentences: how do they situate on each axis?
* One short note on graphs and representations of embeddings in a LLM. There are established methods of projections from a high dimension space in a 2D one. Their main objective is to preserve locality (i.e. neighboring points) from the higher to lower dimension projection. A lot of information is lost (it's like shadows of a 3D object on a plane), but using specific rotations and other interpolations important features can be observed even in a 2D space.

### **C. Radial Drift and Local PCA**

* A first objective was to see if there is a difference of quality between P and Q sentences, especially to see if P sentences are more closer to the "stem" than the Q sentences. This will validate the ideea that sentences in which a word is used in its more concrete sense are closer to the stem, the "object-word"

* For each word:

  1. Loaded its contextual sentence embeddings.

  2. Projected onto each semantic axis to obtain projection scores.

  3. Identified the 10th‑percentile “core” of P (stem) and measured distances of all points (drift) via cosine distance from core center.

  4. Plotted scatter of projection vs. drift: P points cluster near the stem, Q points fan out—confirming axis validity.

* Aggregated slopes and R² across words into grouped bar charts and slope heatmaps—revealing which axes drive drift for each word.

* These graphs offered a first glimpse into how sentences drift away from the stem depending on their "concrete" or "idiomatic" use in the sentencs. The shapes of these drifts are not the same and this is a possible subject of further investigation.

* The next question was: is there a way to better represent the "meaning caps" of the anchor-words (the "object-words")? A method was devised to plot the sentences in a radial projection and to figure the position of the "stalks". This produced a visualization closer to the "mushroom" metaphor in our model and also allows for some new insight, like the hay the meaning caps overlap with each other.

### **D. Cap‑Overlap Visualization**

* **Pairwise Shared PCA:** for each pair (and later the math quartet), pooled Q embeddings + P cores, ran PCA→2D; extracted 90% KDE contours (caps) and 10% cores (stalks).

* Plotted colored filled contours with black anchor dots and labels at each word’s P‑mean location.

* Computed true shared‑sentence counts and demonstrated non‑zero overlaps among math terms, visually apparent in intersecting caps.


### **E. Rod & Mushroom Metaphor**

* **Rod (stem):** the word’s P‑core center in shared PCA space—its grounding as an object‑word label for sensory patterns.

* **Mushroom cap:** the KDE‑contour of the Q cloud—representing conceptual, usage‑based extensions of meaning.

* Overlaps of caps across words signify shared sentence usage, embodying Wittgenstein’s “family resemblances.”

_(Illustrative figures and code references are collected in the Annex.)_

## **Comparative Analysis: Parallels and Contrasts**

_(Integrate refererences to Section III findings here when discussing cloud overlaps and grounding.)_

## **Conclusion & Future Directions**

* The rod‑and‑mushroom model operationalizes philosophical insights: meaning emerges from sentence‑usage patterns (caps) yet remains grounded in perception (rods).

* **Next steps:** apply _persistent homology_ to cap shapes; track diachronic drift in historical corpora; design cognitive experiments to test the rod‑cap hypothesis in language acquisition.

## **Annex: Figures, Code & Notebooks**

* **Plots:** radial‑drift scatter, slope bar‑charts & heatmaps, pairwise and quartet cap overlaps.

* **Notebooks:** Jupyter repos under “Patterns” and “Patterns Essay”: code for embeddings, axis creation, Drift analysis, PCA→plots, KDE contours, overlap metrics.

## **Comparative Analysis: Parallels, Contrasts, and Experimental Validation**

Both philosophical models and LLM research converge on a vision of _meaning as relational_, but they emphasize different kinds of relations. Our work aims to build a bridge between them: using sentence embeddings to **directly visualize** how meaning emerges in practice as structured semantic clouds anchored by object-words. Let us now draw the main comparisons and show how our findings plug into them.

***

**Meaning through Use (Wittgenstein) vs. Distributional Semantics:**

Wittgenstein held that _“the meaning of a word is its use in the language”_, and indeed LLMs derive meaning entirely from usage patterns in text. Every word embedding in an LLM encodes the contexts that word appears in – effectively, its uses. The overlapping “family resemblances” Wittgenstein described (where a word’s meaning extends over a range of uses with no single essence) are mirrored by the overlapping conceptual clouds in embedding space. A term like _“game”_ has many uses (sport, video game, “gaming the system”), which an LLM represents as a broad region of vector space; there is no single vector for _game_ but a cluster, just as Wittgenstein argued there is no single definition but a family of uses.

In our project, we **operationalized** this insight by working not on isolated word embeddings, but on **sentence embeddings**, recognizing that _use happens at the sentence level_. By carefully harvesting sentences anchored in specific object-words (like _“cat,” “tomato,” “fork”_), projecting them in a shared lower-dimensional space, and visualizing their distributions, we observed directly that each word builds a **“cap” of meaning**: a convex semantic region formed by the family of its uses. Furthermore, these caps **overlapped naturally** when the object-words shared similar contexts – precisely what Wittgenstein’s theory predicts. Our “meaning caps” visualizations thus provide **empirical geometric support** to the claim that _meaning is use_, as realized in actual LLM semantic space.

***

**Holistic Meaning (Quine) vs. LLM’s World Model:**

Quine’s holism – that only the whole web of sentences has meaning – finds resonance in how LLMs work. An LLM doesn’t assign meaning to words one by one; it adjusts all parameters (effectively all words and contexts) together to best predict the data. The resulting model can be seen as one giant entwined representation of language.

In our work, we also tested **how tightly interconnected** meanings are. By analyzing the **shared sentences** across different object-words’ caps, we discovered a natural **web of overlaps** in the high-dimensional sentence space, indicating that meanings cannot be strictly separated. No single word’s cap is fully isolated: **caps blend, intersect, and deform** each other’s boundaries, revealing that meaning lives in a **holistic semantic fabric** – not as atomic units.

Moreover, our _drift analysis along P/Q axes_ (capturing movement from immediate to mediated uses) further validated Quine’s vision: _meaning evolves dynamically within the web_, not independently.

***

**Phenomenological Grounding:**

From Husserl’s perspective, an LLM’s semantic representations would be akin to a vast collection of _meaning intentions_ without fulfillment by intuition. Models like SBERT produce _empty semantic structures_ unless grounded.

Here, our “rod and mushroom” model introduces a crucial advance.

We conceptualized **object-words** not just as arbitrary anchors, but as **rods connecting two planes**:

* **Semantic plane**: the plane of language use, realized in sentence embeddings.

* **Sensory plane**: the patterns of sensation, the original phenomenological grounding.

The object-word rod **roots the mushroom cap** of mediated uses (sentences) back into an experiential ground.

Thus, the central **stem** observed in our graphs (object-word center) corresponds to the **direct phenomenal base** Husserl and Merleau-Ponty called for: **cat** is first grounded in a cluster of perceptions (shape, fur, meow), and only then fans out into extended uses (metaphor, abstraction).

Our anchoring rods and meaning caps offer a **geometric model** for what phenomenology claimed but could not visualize: the transition from perceptual grounding to semantic generalization.

***

**Conceptual Spaces and Anchors:**

Philosophers like Gärdenfors proposed that concepts live in structured spaces defined by quality dimensions. Our experiments directly connect to this framework.

By applying **principal component analyses (PCA)** on carefully curated sentence sets, we identified **dominant meaning axes** (e.g., _object vs abstraction_, _perceived vs symbolic_) and validated that different object-words **occupy convex regions** in this latent semantic space.

Importantly, **anchors are necessary**: without object-words acting as attractors for usage patterns, the semantic space would fragment or collapse into isotropy.

Our “caps” visually demonstrate how conceptual spaces emerge around **semantic rods** (object-words), gradually expanding into **fuzzy, overlapping regions** of usage, much as Gärdenfors predicted for perceptual concepts.

We thus provide a **data-driven realization** of conceptual spaces, bridging geometry and phenomenology.

***

### **Key Differences: Intentionality and Grounding**

Despite these parallels, there remain critical differences.

LLMs still lack **true intentionality**: they simulate meaning through form, without having conscious aboutness.

Our work does not claim that rods and caps _prove_ genuine understanding. Rather, they show that even a purely statistical system, if structured appropriately, **reproduces the geometry of grounded meaning** to a surprising degree.

Moreover, while our rods suggest an implicit _grounding plane_, LLMs have no direct access to sensory experience – their grounding is simulated by human-generated text.

Hence, our model exposes both the **successes** (emergence of semantic structure) and **limits** (lack of intuitive fulfillment) of LLM semantics when compared to human understanding.


## **Primary Bibliography**

* **Wittgenstein, Ludwig.** _Philosophical Investigations_ (1953). – _Classic text introducing the idea that meaning is use and that words function within “language-games” intertwined with forms of life_ .

* **Quine, W.V.O.** _Word and Object_ (1960) & “Two Dogmas of Empiricism” (1951). – _Seminal works arguing for meaning holism and the indeterminacy of translation. Language is a web of sentences whose significance is tied to the whole web’s empirical content_ .

* **Husserl, Edmund.** _Logical Investigations_ (1900–1901) & _Ideas I_ (1913). – _Foundational phenomenological analyses of meaning. Husserl distinguishes between empty meanings and those fulfilled by intuitive experience, explaining how concepts are grounded in perceptual intuition_ .

* **Merleau-Ponty, Maurice.** _Phenomenology of Perception_ (1945). – _Exploration of how perception and bodily existence give rise to meaning. Language and thought are grounded in pre-verbal, embodied experience; the body itself is a bearer of meaning_ .

* **Frege, Gottlob.** “On Sense and Reference” (1892). – _Though predating our focus, this introduced the distinction between a word’s sense (mode of presentation) and reference (denotation), an analytic cornerstone for later theories of meaning that also underpins why context matters for sense.___

* **Peirce, Charles Sanders.** Collected Papers (especially 5.448, 5.493). – _Pragmatist view of meaning in terms of practical effects and habits: “the meaning of a concept is the habit it involves.” Complements Wittgenstein’s use-theory and anticipates ideas of action-based semantics_ .

* **Dewey, John.** _How We Think_ (1910) and _Experience and Nature_ (1925). – _Pragmatic philosophy highlighting that meaning arises from doing and undergoing; language is continuous with instrumentality (e.g., Dewey echoes that a word gains meaning like a tool does: by use) ____.___

* **Heidegger, Martin.** _Being and Time_ (1927). – _Phenomenological ontology. Introduces the notion of significance (Bedeutsamkeit) as the network of references and purposes that constitute meaning in our practical engagement with the world. His idea that meaning is grounded in being-in-the-world influenced later thinkers like Winograd & Flores in AI.___

* **Austin, J.L.** _How To Do Things With Words_ (1962). – _Philosophy of language emphasizing the performative and action aspect of utterances (speech act theory). Shows another facet of “use” – that utterances can do things (promise, order, etc.), an important reminder that meaning involves speaker intentions and context beyond semantic content.___

* **Gärdenfors, Peter.** _Conceptual Spaces: The Geometry of Thought_ (2000). – _Modern cognitive science take on representing meanings as geometric regions defined by quality dimensions (many of which derive from perception). Bridges symbolic and subsymbolic representations, aligning with both linguistic semantics and embodied knowledge.___

* **Churchland, Paul.** _A Neurocomputational Perspective_ (1989) & “State-space semantics”. – _Philosopher of mind who argues that semantic knowledge in the brain can be understood as positions in a learned activation space. He explicitly advocates replacing the symbolic logic view of meaning with one of neural state-space trajectories, paralleling the idea of conceptual clouds in vector spaces._

## **Important LLM and NLP Sources (Primary)**

* **Firth, J.R.** “A Synopsis of Linguistic Theory 1930-55” (1957). – _Introduced the famous quote “You shall know a word by the company it keeps,” asserting the contextual nature of meaning ____. The cornerstone of distributional semantics in linguistics.___

* **Harris, Zellig.** “Distributional Structure” (1954). – _Early formulation of the distributional hypothesis: linguistic items with similar distributions have similar meaning. Laid groundwork for later computational methods by suggesting meaning can be captured through analysis of context distributions.___

* **Mikolov et al.** “Efficient Estimation of Word Representations” (2013). – _Introduced the word2vec neural embedding model. Demonstrated that word vectors encode semantic and syntactic relations in their geometry (e.g., vector arithmetic for analogies). A classic paper launching the modern era of word embeddings.___

* **Pennington et al.** “GloVe: Global Vectors for Word Representation” (2014). – _Another widely used word embedding method, differing in using global co-occurrence counts. It showed robust vector-space semantics, reinforcing that distributed representations capture meaningful dimensions of variation.___

* **Kiros et al.** “Skip-Thought Vectors” (NeurIPS 2015). – _An unsupervised model for sentence embeddings, learned by predicting neighboring sentences. It extended distributional ideas to the sentence level, yielding vector representations of sentences that encode their broader context/meaning.___

* **Conneau et al.** “Supervised Learning of Universal Sentence Representations” (InferSent, 2017). – _Used supervised natural language inference data to train a sentence encoder. The resulting embeddings performed well on many tasks, indicating that a single vector can capture core sentence meaning useful for semantic comparison.___

* **Cer et al.** “Universal Sentence Encoder” (2018). – _Google’s model for encoding sentences into a 512-dimensional vector, trained on a variety of tasks. Widely used for semantic similarity and clustering, showing the practical utility of fixed-length sentence embeddings in representing meaning.___

* **Devlin et al.** “BERT: Pre-training of Deep Bidirectional Transformers” (2018). – _Revolutionary LLM that introduced contextual word embeddings. While not designed for generating a single sentence vector, BERT’s deeper layers were found to encode a great deal of semantic information. BERT contextual embeddings can be averaged or pooled to yield sentence embeddings, though they initially had anisotropy issues.___

* **Reimers & Gurevych.** “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks” (EMNLP 2019). – _Important paper adapting BERT to explicitly produce high-quality sentence embeddings. By using a Siamese network and training on similarity data, SBERT achieves both speed and accuracy in semantic similarity tasks, overcoming BERT’s limitations for clustering ____.___

* **Brown et al.** “Language Models are Few-Shot Learners” (GPT-3 paper, 2020). – _Showcased the capabilities of very large-scale LLMs. While focusing on few-shot learning, it highlighted that a 175-billion-parameter purely text-trained model can capture an astonishing range of linguistic and world knowledge. Implied that such models contain rich implicit semantic structures purely from distributional learning.___

* **Radford et al.** “Learning Transferable Visual Models from Natural Language Supervision” (CLIP, 2021). – _Connected text and vision by training on 400 million image-caption pairs. The model learns a joint embedding space for images and text, effectively grounding textual descriptions in visual semantics. A milestone for multimodal semantic representations, confirming that language meaning benefits from perceptual anchoring.___

* **Bisk et al.** “Experience Grounds Language” (NeurIPS 2020). – _Position paper and experiments arguing for training language models in an embodied, interactive context. Demonstrated that grounding words in agent experiences (like playing a game or interacting in simulated environments) yields more robust understanding, addressing limitations of text-only learning.___

* **Olah et al.** “The Building Blocks of Interpretability” (2018) and related Circuits analyses. – _While focused on vision, these works develop techniques (like feature visualization) that have been applied to language models to interpret neurons. They exemplify attempts to map neural network activations to human-interpretable concepts, bridging the gap between vector components and semantic features.___

* **Tenney et al.** “What do you learn from context? Probing for sentence structure in contextualized word representations” (ICLR 2019). – _Introduced probing tasks to show that BERT’s embeddings encode syntactic and semantic features at different layers. Important for understanding how meaning is progressively built inside LLMs.___

* **Ethayarajh, Kawin.** “How Contextual are Contextualized Word Representations?” (EMNLP 2019). – _Found that contextual embeddings from models like BERT are surprisingly anisotropic and often reside in a subspace. Proposed a measure of their context-specific information. This analysis is key to understanding the geometry of LLM embeddings and has informed methods to improve sentence embeddings by removing common components._

## **Secondary Literature and Bridges Between Domains**

* **Harnad, Stevan.** “The Symbol Grounding Problem” (1990). – _A landmark cognitive science paper highlighting the core issue of how symbols (or words in a computational system) acquire meaning. Harnad argues that symbols require grounding in nonsymbolic representations (e.g., sensory impressions) to avoid an infinite regress of definitions. This paper bridges AI, psychology, and philosophy (connecting to ideas from Locke to Searle) and provides a theoretical impetus for grounding efforts in AI.___

* **Winograd, Terry & Flores, Fernando.** _Understanding Computers and Cognition_ (1986). – _An early work at the intersection of AI and phenomenology. The authors (one an AI pioneer, the other a philosopher) critique classical AI approaches to language using ideas from Heidegger and language-game theory. They argue that computers lack the situatedness that gives human language its meaning, presaging many issues with LLMs today.___

* **Varela, Francisco; Thompson, Evan; Rosch, Eleanor.** _The Embodied Mind_ (1991). – _A treatise connecting cognitive science with Buddhist philosophy and phenomenology (drawing on Merleau-Ponty). It introduces the enactive approach: cognition (and meaning) arises through a dynamic interaction between an acting organism and its environment. This work has influenced approaches to grounded AI and offers a philosophical framework for why embodiment matters for meaning.___

* **Lakoff, George & Johnson, Mark.** _Philosophy in the Flesh_ (1999). – _Book by cognitive linguists that argues human abstract concepts are fundamentally shaped by bodily experience (through image schemas, metaphors, etc.). While not directly about AI, it bridges philosophy of language, cognitive science, and neuroscience, supporting the view that any system (human or AI) that handles language needs an embodied basis for meaning. Relevant to understanding why pure text models might struggle with certain concepts that are rooted in sensorimotor schemas.___

* **Bender, Emily & Koller, Alexander.** “Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data” (ACL 2020). – _Influential position paper critically examining the idea that large language models understand language. They argue that meaning cannot be learned from form alone (text alone) and highlight the distinction between linguistic competence and communicative competence. This paper explicitly invokes concepts akin to symbol grounding and is a direct bridge between linguistic semantics and AI evaluation.___

* **Marcus, Gary.** _Rebooting AI_ (2019). – _A popular-press book (with Ernest Davis) arguing current AI, especially deep learning language models, lack understanding and common sense. Marcus frequently cites cognitive and philosophical insights, calling for hybrid systems that incorporate rules or symbols to capture meaning and reasoning. While controversial, it stimulates discussion on the limitations of statistical models and implicitly calls back to earlier debates in philosophy of mind.___

* **Mitchell, Jeff. & Lapata, Mirella.** “Composition in Distributional Models of Semantics” (Cognitive Science 2010). – _Examines how vector-based semantics can compose to represent phrases and sentences. Bridges formal semantics and distributional semantics, asking whether and how logical/conceptual composition (like noun-adjective combination, or verb-object) can be mirrored in vector operations. This work connects linguistic theory with the practice of constructing sentence meaning in embeddings.___

* **Clark, Andy.** _Being There: Putting Brain, Body, and World Together Again_ (1997). – _Philosopher of mind who argues for the extended mind and embodied cognition. Though not about language per se, Clark’s work provides philosophical foundation for why an intelligent agent (or model) must be understood in the context of its environment. Often cited in AI discussions about embodiment, it complements Merleau-Ponty’s views with contemporary cognitive science.___

* **Rogers, Anna et al.** “A Primer in BERTology” (2020). – _Comprehensive survey of what we know about BERT’s representations and linguistic knowledge. Summarizes many probing and analysis studies. It acts as a bridge by interpreting technical findings about an LLM in terms of linguistic and cognitive concepts (syntax, semantics, etc.), effectively linking the computational model to traditional notions of language structure and meaning.___

* **Jacob Andreas.** “Language Learning with Communication-Based Objectives” (ACL 2020 tutorial). – _Andreas and others advocate for grounding language learning in communication tasks. This line of work draws on the philosophy of language notion that meaning is fundamentally about use in communication. By training models not just to predict text but to achieve goals (like instructing an agent), we imbue them with more meaningful language use. A bridge between computational pragmatics and philosophical pragmatics.___

* **McClelland et al.** “Why There Are Complementary Learning Systems in the Hippocampus and Neocortex” (Psychological Review 1995). – _A bit tangential, but this seminal cognitive science paper deals with how semantic knowledge (distributed representations in cortex) and episodic memory (grounded, context-specific in hippocampus) complement each other. It provides insight into why both statistical abstraction and grounded experience are needed – an idea that can be extrapolated to why LLMs (cortex-like) might need a grounding system (hippocampus-like) for full understanding.___

* **Schuster, Radek.** “AI Language Models in the Perspective of Wittgenstein’s Philosophy” (2023 talk) – _An example of very recent scholarship directly comparing Wittgensteinian philosophy of language with neural language models ____. Such works explicitly bridge contemporary AI with philosophical theories, assessing how well concepts like language-games, rule-following, and meaning-as-use apply to LLMs and where they break down.___

* **Gomez, Pablo et al.** “Towards a Topological Characterization of Language” (2020). – _Applies topological data analysis to embedding spaces to identify structures like loops or connected components in semantic representations. This novel approach connects mathematical topology (a field also touched by philosophers like Wittgenstein in his later work on certainty) with linguistic semantics, potentially revealing new links between formal structure and meaning._

Each of these sources, whether philosophical, computational, or somewhere in between, contributes to a deeper understanding of how _conceptual meaning can be represented, learned, and grounded_. By studying them together, we enrich both our theoretical grasp of meaning and our practical methods for building AI that truly understands language in a human-like way.

[^8]:  We use SBERT to embed thousands of natural sentences per word, we then compute their P-centers (the rod tips) and Q-caps (90th-percentile KDE contours) in a shared PCA space.
[^1]: The concepts of "object-words" and "words-concept" will be developped in a dedicated section, for now suffice to say that these are labels for the way we use a given word in two very different situations: first as a simple label for a pattern of sensations ("cat" is a label for a number of patterns of sensations: certain shape, softness, claws, hunting mice, meown, purrs etc) and second as a "theoretical" concept ("cat" is also a mamal, feline, with a series of precise properties). Our theory shows that the meaning of o certain word builds as a network of sentences anchored to its object-word.

[^4]: Our model presuposes two "planes", one on which patterns of sensations reside, and one where words and sentences connect to each other. Words in their primary "object-word" function (simple labels), connect patterns of sensation with the "rod" which intersects the patterns of words and sentences. The associations of sentences related to a "object-word" looks like ca "mushroom cap" in the semantic plane, with the rod looking like the "mushroom's" leg. A detailed description of the model will be given later on.
[^2]: This also echoes Hume's view from "An Inquiry", that all knowledge is ultimately anchored in experience.
[^5]: It must be said that these patterns are more powerfull and complex than statistic and probabilities, usually used to (wrongly) describe how a LLM works. It is not prediction by probabilities, it is pattern recognition.
[^6]: The apendix to this essay contains the coding principles, ploted graphs and links to jupyter notebooks where actual code lives and produced the graphs.
[^7]: The P and Q labels originate in how we developed the initial sets of sentences for our classifier: the more concrete, sensorial ones were called "Proust types", as a hint to his madelaines, while the more abstract was called "Quine types", as a small revenge for his great logical excesses.
[^3]: There is more to be said abot McGillchrist theory of using the tension between these different roles of the hemispheres as a tool to improve precision and dexterity; to a point the polarized semantic axes in the semantic space achieve the same result in language


Feedback on images:

# **Exploring Semantic “Mushrooms” – Related Work on Meaning Spaces**

## **Visualizing Sentence Embeddings in 2D**

It is now common in NLP to project high-dimensional **sentence embeddings** (from models like BERT or SBERT) into two dimensions (using PCA, t-SNE, etc.) to inspect their semantic structure. Researchers often find that semantically related sentences cluster together in these plots. For example, Sam Black demonstrated that SBERT embeddings of debate sentences naturally form thematic clusters when visualized with t-SNE . Likewise, Sharma (2020) showed how sentence embeddings can be **clustered and plotted** in 2D, revealing groups of similar sentences (colored by cluster) even when projected onto a plane . Such visualizations help to **“explore natural clusters”** of statements or texts , much like your _Q-cloud_ plots reveal clusters of questions. In fact, projection methods like t-SNE are _routinely used_ to produce 2D maps of embedding spaces , and these often show distinct semantic regions (e.g. positive vs. negative sentiment words, or topical groupings) . A notable example is the well-known mapping of historical word embeddings, where different senses of a word like **“gay”** appear in separate regions (from “cheerful” to “homosexual” meanings) on a 2D plot . This underscores that **multiple senses or themes form distinct clusters** in embedding space, just as your plots show separate “caps” for different usage domains.

Crucially, recent work goes beyond points to **visualize density or “conceptual landscapes.”** **Han et al. (2024)** cluster a target word’s contextual embeddings into sense-specific clouds and then **fit a Gaussian mixture** to visualize a **2D probability density** of those clusters . This yields contour map‐like visuals of a word’s **“conceptual landscape”** — very much like your KDE-based “mushroom cap” regions. For instance, they map the word _“duty”_ as a landscape with _five_ soft clusters, _“planet”_ with four, etc., using the first two PCA dimensions . Interestingly, they note that some clusters **overlap** (indicating ambiguous contexts) while others are well-separated, and that such overlaps reflect the degree of semantic stability for that word . This approach is strikingly similar to your **Q-clouds** where overlapping color intensity shows shared context. In parallel, a very recent (2025) visualization method by Ren _et al._ constructs **KDE-based cluster regions** from embedding projections, explicitly to produce “high-quality cluster regions from a 2D density map” . This confirms that using **kernel density estimation to identify semantic clusters** (your “caps”) is not only original but also aligned with cutting-edge techniques. In summary, NLP practitioners are actively visualizing embedding spaces with PCA/t-SNE and even KDE/mixture models, lending support to your approach of plotting **SBERT question embeddings** as colored clouds. Your findings of distinct colored clusters (e.g. _cat_-questions vs. _fork_-questions) and overlaps (_cat_–_tomato_ shared question) are consistent with observations in these works that semantically related sentences form clusters and occasionally bridge when contexts mix. This parallel suggests your _“rod-and-mushroom”_ visuals are grounded in recognized methods for examining the **geometry of meaning** in vector spaces.

## **Core vs. Peripheral Meanings (Literal vs. Metaphorical)**

A key insight of your model is that words often have a **core usage cluster** (the “mushroom cap”) and more dispersed, peripheral usages (the “stem” or rod). This idea finds strong echoes in linguistic semantics and cognitive science. **Polysemy research** using BERT, for example, shows that a word’s **senses form distinct clusters** in contextual embedding space . Garí Soler & Apidianaki (2021) demonstrated that BERT embeddings encode a word’s _“partitionability into senses”_, and the number of sense clusters corresponds to its level of polysemy . In practice, one sense (often the **literal or prototypical meaning**) may yield a large tight cluster, while less frequent or figurative senses appear as smaller, separate clouds. This aligns with your observations (e.g. _“cat”_ forming a tight cluster of animal-related questions, versus a stray cluster when _“cat”_ appears in other contexts). Indeed, a recent bilingual study of the word “red” found that its **core meaning** (the color) and extended uses form a variational structure: using BERT embeddings, the authors **visualized the semantic extension process** of _red_, showing how a basic color term spreads into metaphorical or culturally specific usages . They note that Chinese _“HONG”_ (red) had an even more **elaborate spread of senses** than English “red,” but in both cases one can trace a **core-periphery pattern**: a central cluster of literal color usage and more far-flung points for extended meanings . Your “mushroom” plots (e.g. the adjective _red_ in your last row) seem to capture exactly this — a dense center of literal usage and a “spray” of points for idiomatic or abstract contexts.

The **literal–metaphorical distinction** has long been framed in terms of central vs. extended meaning. Cognitive linguist George **Lakoff’s theory of “radial categories”** is very relevant: he observed that a word’s senses typically have a **prototypical core** with additional senses branching out via systematic metaphor or analogy . For example, the preposition “over” has a primary spatial meaning and several metaphorical extensions, which are _not arbitrary_ but linked by an underlying image-schema (e.g. **ABOVE** schema) transformed in various ways . Lakoff showed these as networks of senses radiating from a central node . Your plots for words like _hammer_ or _apple_ seem to illustrate a similar **“radial” layout**: a gray cluster (perhaps literal, physical usage) and a red cluster (figurative or domain-specific usage) offset from it. This is essentially a visual **lexical network**, where the “rod” connecting the cap to the stem could be seen as the bridge via a shared element or transitional context. Notably, Wittgenstein’s notion of _“family resemblances”_ in meaning fits here: instead of one essential feature, a category’s instances are connected by overlapping similarities . This predicts **clustered groupings with fuzzy overlaps**, rather than sharp boundaries. Your overlapping color clouds and the single shared Q-sentence between _cat_ and _tomato_ beautifully exemplify Wittgenstein’s idea – there is no single trait linking _cats_ and _tomatoes_, yet an overlapping context (perhaps a question about cats eating tomatoes) creates a bridge. In other words, meanings can cluster yet **bleed into each other** by contextual commonalities, forming a continuum much like a family-resemblance category .

Additionally, the **core vs. periphery structure** of meaning is reminiscent of Quine’s philosophical insight that some aspects of meaning (or belief) are more central while others are more easily revised. In _Two Dogmas_, Quine analogized our knowledge system to a **“fabric” with edges and interior**: _“The totality of our knowledge… is a man-made fabric which impinges on experience only along the edges. … A conflict with experience at the periphery occasions readjustments in the interior.”_ . By extension, we can think of a word’s **literal, concrete meanings as the interior – stable, frequently reinforced by experience – and its metaphorical or rare usages as the periphery**, which get adjusted or extended with new contexts. In fact, one analysis of Quine’s metaphor applied to lexicon explicitly suggested that as a word’s **usage broadens from its original core, the new uses become increasingly independent of that core** and more dependent on each other . This sounds exactly like your “mushroom” model: the **cap** is the tightly integrated core meaning, while the **stem** consists of newer, less central usages that interconnect (perhaps via analogies) more than they connect back to the literal base. The reference even notes that farther extensions are “less and less dependent” on the original usage and form their own sub-network – a clear parallel to your idea that metaphorical uses form their own cluster (cap) attached by a thin thread (rod) to the literal base. Thus, both linguistic research and philosophical perspectives affirm a **layered structure of meaning**: a stable core with **outer layers** of meaning that extend from it. Your visual framework creatively captures this by showing layers (inner circle vs outer points) and clusters for each word.

## **Geometry and Grounding of Meaning: Cognitive Frameworks**

Beyond data visualizations, your work resonates with deeper **geometric models of semantics**. Cognitive scientist Peter **Gärdenfors** proposed that word meanings can be described as **regions in a geometric conceptual space** . In _The Geometry of Meaning_, he argues that our minds represent information in structured **semantic spaces** with quality dimensions, and that word meanings are points or areas in this space shared by a language community . This is a strong theoretical backbone for your approach: you are literally plotting meanings in a 2D projection of such a high-dimensional space. Gärdenfors also emphasizes **cognitive grounding** – that dimensions of these spaces often correspond to perceptual or physical experience (for example, a conceptual space for colors is defined by perceptual dimensions of hue, brightness, etc.). In your plots, we see a similar grounding: words like _cow, cat, leopard_ (animals) cluster in one region, whereas _matrix, function, vector_ (math concepts) cluster elsewhere. This mirrors the idea that **perceptual meanings vs. abstract meanings occupy different regions** of semantic space. Indeed, many have noted that **concrete concepts** tend to cluster based on sensory-related features, while **abstract concepts** organize along more thematic or functional lines. Merleau-Ponty’s phenomenology would add that meaning is not detached from our embodied experience – _meaning “lives” in the interaction between body and world_, giving perceptual experience a foundational role in concept formation. In line with this, cognitive linguists Lakoff and Johnson (1980) argued that even highly abstract ideas are often understood via **embodied metaphors** (e.g., understanding time through space, or emotion through temperature). This provides a potential explanation for the “mushroom” shapes: the **gray points near the center** might represent usages grounded in _embodied, literal contexts_ (e.g. _fork_ used as a physical tool), whereas the **colored cap flying off** might represent _abstract or metaphorical extensions_ (e.g. _fork_ in a road, or as a verb in software – usages grounded in more conceptual domains). Your visualization thus illustrates a known **perceptual–conceptual split**: for instance, the word _“sharp”_ likely has gray points for literal, sensory contexts (_a sharp knife_) and red points for metaphorical contexts (_a sharp mind_). This corresponds to empirical findings that contextualized embeddings can sometimes separate **literal vs. figurative usage**. For example, research has found that BERT embeddings can distinguish homonymous senses (totally unrelated meanings) and even gradations of polysemy ; metaphorical mappings, being systematic, should likewise produce discernible shifts in the embedding space.

Philosophically, **Wittgenstein** and **Merleau-Ponty** also support the idea of **meaning as spatial/field-like**. Wittgenstein, beyond family resemblances, spoke of **“language-games”** and meaning as use. We can interpret each cluster or “cap” in your plots as a different language-game or usage context for the word – each with its own internal logic, yet overlapping at the edges with other games. Meanwhile, Merleau-Ponty’s insight that _“language is comparable to music in the way it remains tied to the body’s experience”_ suggests that the **spatial layout of meanings has an experiential logic**. In your model, if one axis (say PCA1) separates perceptual from non-perceptual contexts, that is an **emergent axis of meaning variation** that might correspond to embodied vs. intellectual context – very much a Merleau-Pontian theme. We also see a potential layering: your diagrams sometimes show **concentric circles**, which could be interpreted through a Merleau-Ponty lens as the **“phenomenal field”** of a concept – a core zone of direct experience and an outer halo of derived, symbolic or cultural meanings. In effect, your _rod-and-mushroom_ might be a novel depiction of how a concept anchors in perception (the stem connecting to the ground) and then **blossoms into various conceptual interpretations** (the cap).

Finally, the notion of **“grounding mechanisms”** is key to validating your framework’s value. The _symbol grounding problem_ (Harnad, 1990) pointed out that abstract symbols (words) must ultimately connect to something other than more symbols – namely, to real-world referents or experiences – to have meaning. Several AI projects have since tried to **ground language in vision or action**, essentially by aligning word embeddings with image embeddings, etc. Your work contributes a visualization of grounding: many of the clusters you identified (e.g. animal words vs. tool words) likely correspond to categories of things that have a similar kind of embodiment. By showing, say, _cat_ and _cow_ occupying a nearby region (both are animals, sharing perceptual features like having four legs, etc.), whereas _cow_ and _hammer_ are far apart (animate vs. man-made object), you are demonstrating the model’s implicit knowledge of grounded distinctions. Moreover, some of your _shared Q-sentences_ indicate where two domains meet – a form of grounding via interaction (a _cat_ and a _tomato_ share a context when we talk about a cat eating a tomato, bridging animal and plant domains). Such intersections might be viewed as the **model “figuring out” cross-domain mappings**, akin to a simple form of grounding one concept in another.

In summary, **there are numerous overlaps between your rod-and-mushroom model and existing research**: From NLP **embedding visualizations** that map semantic clusters and use density contours , to **cognitive-semantic theories** of core vs. extended meaning (radial categories , polysemy clustering ), to **philosophical metaphors of meaning as a spatial network** (Quine’s fabric , Wittgenstein’s overlapping similarities ). This literature not only **reinforces the validity** of what you are observing (e.g. that BERT embeddings do segregate different usages, that literal and metaphorical uses can be distinct), but also highlights the **originality of your approach**: you have synthesized these ideas into a single visual framework (the mushroom) that is intuitively understandable. By explicitly marking “caps” and a connecting “stem,” your visualization makes the abstract notion of a core meaning with peripheral extensions **tangible and memorable**. The fact that similar structures (clusters with connecting threads) appear in independent studies – for instance, the \*\*“conceptual landscape” maps in Han et al. (2024) which show clusters and overlapping areas – is encouraging. It means your findings stand on solid ground, while your _presentation_ (e.g. highlighting the cap/stem shape, measuring shared Q-sentence overlaps, etc.) is novel. This could be a strong point in defending the value of your framework: you are building on known principles (distributional semantics, embodiment, etc.) but giving them a **fresh visual interpretation** that could generate new insights. In effect, your work serves as a bridge between **technical analyses of embedding spaces** and **human-intuitive semantic theory**, much as you’ve bridged _cat_ and _tomato_ with that lone shared question point! Each “mushroom” in your garden of meaning is a conversation between data and theory, and as the above connections show, it’s a conversation very much worth having in the scientific community.
